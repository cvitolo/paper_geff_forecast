---
title: "Fire danger: the skill provided by the ECMWF Integrated Forecasting System"
journal: "`r rticles::copernicus_journal_abbreviations(journal_name = 'essd')`"
author:
  - given_name: Francesca
    surname: Di Giuseppe
    affiliation: 1
    email: F.DiGiuseppe@ecmwf.int
    corresponding: true
  - given_name: Claudia
    surname: Vitolo
    affiliation: 1
  - given_name: Blazej
    surname: Krzeminski
    affiliation: 1
  - given_name: Jesus
    surname: San-Miguel
    affiliation: 2
  - given_name: Florian
    surname: Pappenberger
    affiliation: 1
affiliation:
  - code: 1
    address: European Centre for Medium-range Weather Forecasts, Reading, United Kingdom
  - code: 2
    address: European Commission, Joint Research Centre, Ispra, Italy
abstract: "In the framework of the EU Copernicus program the European Centre for Medium-range Weather Forecast (ECMWF) on behalf of the Joint Research Centre (JRC) is forecasting daily fire weather indices using its medium range ensemble prediction system. The use of weather forecast in place of local observations can extend early warnings up to 1-2 weeks allowing for greater proactive coordination of resource-sharing and mobilization within and across countries. In addition, the use of an ensemble system allows to perform a probabilistic assessment of the forecast uncertainties which can boost confidence in the decision process during emergency situations. Using one year of pre-operational service in 2017 here we assess the capability of the system globally and analyze in detail three major events in Chile, Portugal and California. We also present examples on how fire forecast products could be tailored to provide uncertainties of fire weather conditions and information in a probabilistic fashion."
bibliography: references.bib
running:
  title: Fire danger forecasting
  author: Di Giuseppe et al.
# This section is mandatory even if you declare that no competing interests are present.
competinginterests: |
  The authors declare no competing interests.
availability:
  code: "The code to generate the results of this work is available as vignette of the caliver R package: Verification of fire danger classes."
authorcontribution: |
  FDG wrote the ...
acknowledgements: |
  This work was founded by the EU Project ANYWHERE (Contract 700099) and the Global Fire Contract 389730 between the Joint Research Centre and ECMWF.
output:
  rticles::copernicus_article: default
  bookdown::pdf_book:
    base_format: rticles::copernicus_article
---

\introduction

The prediction of fire danger conditions allows forestry agencies to implement fire prevention, detection, and pre-suppression action plans before fire damages. However, in  many countries fire danger rating relies on observed weather data which only allows for daily environmental monitoring of fire conditions \citep{taylor:06}. Even when this estimation is enhanced with the combined use of satellite data, such as hot spots for early fire detection, and land cover and fuel conditions it normally only provides 4- to 6-hour warnings. By using forecast conditions from advanced numerical weather models, early warning could be extended up to 1-2 weeks allowing for greater coordination of resource-sharing and mobilization within and between countries. 

Due to the improved skills of weather forecasting, the use of numerical weather prediction offers a real opportunity to enhance early warning capabilities \citep{roads:05,molders:08,molders:10}. In recent years institutions such as Natural Resources Canada (NRC) and the US National Oceanic and Atmospheric Administration (NOAA) have implemented regional fire danger forecasting systems based on their operational weather forecasts \citep{bedia:18}. The  Global Fire Early Warning System is also an international initiative, promoted by the Canadian Parternship for Wildland Fire Science and the United Nation Office for Disaster Risk Reduction, to provide fire danger forecast up to 10 days ahead using the Canadian operational weather forecasting system (\url{http://canadawildfire.ualberta.ca/gfews}). Parallel initiatives are promoted by the European Commission under the umbrella of the Copernicus Emergency Management Service (CEMS), namely the European Fire Forecast Information System (EFFIS, \url{http://effis.jrc.ec.europa.eu/}) and its global counterpart the Global wildfire Information System (GWIS, \url{http://gwis.jrc.ec.europa.eu/}). Both systems rely on the Canadian Fire Weather Index (FWI) \citep{vanwagner:74,vanwagner:85} to rate fire danger and on numerical weather predictions to provide forecasted fire danger information at the European and global levels \citep{sanmiguel:02}.

Systems such as the FWI detect dangerous weather conditions conducive of uncontrollable fires rather than modelling the probability of ignition and fire behaviours. The FWI (developed in Canada) is specifically calibrated to describe the fire behaviour in a jack pine stand (*Pinus banksiana*) typical of the Canadian forests. However, its simplicity of implementation has made it a popular choice in many countries and it has shown to perform reasonably well in ecosystems very dissimilar to the boreal forest \citep{digiuseppe:16,degroot:07}. The FWI calculation only relies on weather forcings and no information on the actual vegetation status is taken into account. When weather forecasts are used in place of observations, uncertainties can be introduced. Sources of uncertainty can be: (i) the limited predictability of atmospheric dynamics which is strongly affected by the initial state and (ii) the misrepresentation of physical processes. In the former case, errors are randomly distributed around the true state \citep{orrell:01}; in the latter, errors produce systematic deviations from the true state. In both cases, errors in the weather forecast may be amplified or damped by nonlinear transformations in the fire weather model \citep{erickson:18}. Thus, for example, a dry bias in the model in a certain region will lead to the persistent prediction of higher fire danger values compared to what would be calculated using local observations. 

Handling random errors in weather forecasts is traditionally done through the use of ensemble prediction systems where several simulations are performed starting from slightly different initial conditions and model configurations \citep{molteni:96,buizza:99}. The forecast is then interpreted as probabilistic rather than deterministic. While it has been shown that the probabilistic information contained in an ensemble prediction system might be difficult to interpret for end-users \citep{pappenberger:13}, ensembles can also boost confidence in the decision process during emergency situations as a cost-loss analysis can be associated to the different scenarios \citep{cloke:17}. Moreover, ensemble predictions can have more information value than the single deterministic simulation \citep{richardson:00,zhu:02}.
Systematic biases, on the other hand, can be reduced by model improvements. Still appropriate post-processing (bias correction) of the atmospheric model \citep{piani:10,digiuseppe:13a,digiuseppe:13b} or post-processing of the sectoral application outputs \citep{raftery:05} can correct resolved processes and improve the final forecast skill. 

Given the above considerations, in this paper we assess the performance of the fire danger forecasting system developed for the Copernicus Emergency Management Service at European Centre for Medium-range Weather Forecasts (ECMWF) to predict the FWI values and the probability of detection of fire during one year of operation in 2017. As the Fire Weather Index is the main index of this system we will concentrate on this model component. 

\subsection{FWI calculation}
\subsubsection{General concept}
The Fire Weather Index system is composed of six variables describing fuel moisture and fire behavior, as influenced by weather \citep{vanwagner:87}. Three fuel moisture codes provide the moisture content of dead woody debris of different diameter classes laying on three fuel beds. The dead fine fuels is described by the Fine Fuel Moisture Code (FFMC) which has a fast response time to weather modification (hourly time scales). The surface organic matter of moderate density, such as the fermentation layer of forest soils \citep{hood:10} is calculated using the Duff Moisture Code (DMC). DMC has a slower response to the weather forcing, conventionally set to 12 days. Finally the deep, compact soil organic layers, such as the humus layer of forest soils \citet{hood:10} is expressed with a Drought Code (DC). This is the slower evolving component of the FWI codes and has a lag-time set to 52 days in the original FWI formulation. The FWI system also provides fire behavior indexes, in terms of rate of fire spread (Initial Spread Index, ISI), fuel available for combustion (Buildup Index, BUI), and head fire intensity (Fire Weather Index, FWI). The FWI component combines the ISI and BUI and can be considered as the most general indicator of fire danger.

A comprehensive description of the FWI system, the interaction between the various components and how these are used in fire management can be found in \citep{vanwagner:87,wotton:09}. In the interest of brevity we have here only recalled the basic concepts underlying the FWI system and how different components might be more appropriate to describe fire danger depending on the local fuel characteristics and the type of information required. \cite{abatzoglou:18} showed that FWI exhibits strong correlative relationships to burned area across most non-arid eco-regions globally, while \citet{bowman:17} highlighted how high FWI values are often associated to the most extreme fire activities recorded using Fire Radiative Power observations. As FWI has been shown to provide a good metric for quantifying fire danger globally, the proposed analysis of forecast skills will concentrate on this index \citep{digiuseppe:16,degroot:07}. 

\subsubsection{FWI forecast}

For each day indexes of the FWI rating system are calculated operationally at ECMWF using real-time (RT) forecasts. A full description of the modeling components can be found in \citet{digiuseppe:16}. The high resolution (HRES) and the ensemble prediction systems (ENS) provide weather forecasts which extend up to 10 days in the future. The atmospheric forcings have a temporal resolution of 3 hours and a spatial resolution of 9km for the high resolution run and 18 km for the ensemble prediction simulations. While the HRES is a single (deterministic) model integration, the ENS provides 51 realizations from perturbed initial conditions and different model physics \citep{buizza:99}. These ENS forecasts are used to assess uncertainties in the prediction. 

A model integration at any nominal time simulates atmospheric conditions at a different local time, depending on the location. FWI calculations are usually performed at 12:00 local time because the model was calibrated using  measurements at 12:00 against fire behavior the most active window (between 14:00-16:00) \citep{vanwagner:87}.  Therefore to produce a snapshot at 12:00 local time, a temporal and spatial collage of 24 hours time model simulations is performed. Atmospheric fields are cut into 3-hourly time strips using the closest 3-hour forecast outputs and then concatenated together so that the final field is representative of the conditions around the local noon within the 3 hour resolution available \citep{digiuseppe:16}.

\subsubsection{FWI reference and benchmark}

As many forestry agencies still rely on observed meteorological data to provide fire danger, a first assessment of the quality of forecasted FWI will rely on the comparison with observations. Despite several meteorological observations are available through the Global Telecommunication System (GTS) SYNOP network, only a subgroup of stations have at least 30 days of recordings at local noon during 2017 (spatial coverage is given in figure \ref{fig:synop}). Many fire prone regions, such as Australia, would not be covered by this comparison. In order to overcome this limitation, a reference dataset of FWI modelled values is also used. This is constructed using the ERA5 reanalysis dataset. ERA5 is the latest of ECMWF reanalysis products which was released at the beginning of 2019. It substitutes the previous ERA-Interim database \citep{dee:11,vitolo:19} providing a much improved spatial resolution and an extensive increment of assimilated observations. Simulations begin in 1979 and are updated in quasi real time. Fields have a spatial resolution of about 30 km and hourly time resolution. Outputs from ERA5 undergo the same temporal interpolation described in the previous section to provide the model with a composite fire reanalysis product at 12:00 local time. It has to be noted that, compared to local observations, a reanalysis provides a dynamically consistent estimate of the climate state at each time step and can, to a large extent, be considered a good proxy for observed meteorological conditions. Moreover, by  combining different observations, reanalysis datasets extend well beyond the natural life of single observational networks and they can provide a wider spatial coverage than using local observations.

In addition, from ERA5 we derive a climatological benchmark simulation (called CLIM hereafter), considered as a "mean year" for the FWI using the period 1980-2018. At pixel level, every day the median FWI is calculated over the period spanning 4 days before/after a given date. While CLIM has no expected predictive skill for a specific year, it retains information on the yearly climatological variation of FWI. CLIM is what could be used in the absence of either a monitoring (based on observations) or a forecast system (based on weather forecast). It should therefore score better or equal of the forecast on time ranges beyond the limits of predictability. CLIM is used in this study as a benchmark to rank the expected improvements provided by a forecasting system.

\subsection{Observed fire events}

While national inventories of wildfire activities exist in many countries, they can be heterogeneous and lack the temporal span desirable for the validation of a fire danger system at the global scale. Satellite observations can supply a valid alternative especially as they cover remote areas where in-situ observations are sparse \citep{flannigan:86,giglio:03,schroeder:08}. Daily maps of fire radiative power (FRP) \citep{kaufman:03,wooster:05} are available from ECMWF since 2003 through the Global Fire Assimilation System (GFAS) \citep{kaiser:2012,digiuseppe:17,digiuseppe:18}. This dataset has been developed in the framework of the Copernicus Atmosphere Monitoring Services (CAMS) and uses observations from the MODIS sensors on board of Terra and Aqua platforms and assumptions on fire evolution to calculate a continuous record of active fires. The GFAS dataset integrates all available FRP observations available in a day over a regular $0.1\deg$ grid. According to \citet{wooster:05}, this provides an indication of the cumulative dry mass available for burning which can be then put into a relationship with fire emissions. In this paper, the FRP products are only used as an observations of fire events. However, FRP values are ignored and only used to derive a mask of fire occurrence based on a minimum detection criteria: $FRP> 0.5 Wm^{-2}$ \citep{kaiser:2012}. A "hit" is recorded if the fire forecast predicts fire danger above the 95\textsuperscript{th} percentile of its historical values (provided by the ERA5 simulations) when a fire really occurred.

\subsection{Score metrics}

The performance of the the fire forecasting systems to reproduce observed FWI values is assessed using deterministic and probabilistic scores. Both the  synop database and ERA5 are  treated as a proxy for observations in the evaluation. To asses the quality of the computation we use traditional deterministic skill scores such as the  mean bias (MB) and the  mean absolute  error (MAE). For a probabilistic assessment, the continuous ranked probability score is also employed (CRPS; \cite{hersbach2000}). These are defined as:
\begin{align}
MB&=\sum_{p=1}^{cases}\left[F_{n=HRES}-O\right]\\
MAE&=\frac{1}{cases}\sum_{p=1}^{cases}\sqrt{\left[(F_{n=HRES}-O)^2]\right]}\\
CRPS=\frac{1}{cases}\sum_{t=1}^{cases}\int_{-\inf}^{+\inf}\left[F_{n}-O)^2\right]dn
\end{align}
where F is the forecast at time step t of N number of forecasts and O is the observed value. While the MB, MAE and the ACC are applied to a single forecasts, the high resolution forecast,  the CRPS takes into account the whole distribution of possible values predicted by the ensemble. It is the continuous extension of the ranked probability score, where $F_n$ is the cumulative distribution function of the predicted ensemble values. Then, the CRPS compares the cumulative probability distribution of the FWI forecasted by the ensemble forecast system to the observation. In this sense the CRPS is sensitive to the mean forecast biases as well as the spread of the ensemble \citep{hersbach2000}.

While conventional skills score can be employed to assess the quality of the FWI computation, the  verification of the FWI as a fire indicator is instead extremely challenging. First, as widely explained, FWI is not a physical measure of fire activity but of its potential danger, if one were ignited. Therefore high fire danger, while being correctly forecasted, might not result in active fires if there is no ignition. From the verification point of view this means that the identification of false alarms is not meaningful and the verification should mainly rely on hits and misses. Secondly, fires are rare events and, as for any other infrequent phenomena, the verification statistics are heavily influenced by the small number of hits when compared to the total. Still, when the cost of a missed event is high, for example in terms of human lives, the deliberate over-forecasting may be justified \citep{richardson:00,cloke:17}.

In these cases a positively oriented score such as "hit rate" may be more useful. Also forecast quality does not always equals forecast value \citep{richardson:00}. A forecast has high quality if it predicts the observed conditions well according to some objective or subjective criteria. It has value if it helps the user to make a better decision in terms of protective actions \citep{cloke:17}. For example predicting high temperature and low precipitation in desert areas might be accurate but carries low information content and therefore limited value. Following these arguments and to gain an appreciation of the potential value of the forecasting system globally we use as a metric the Probability of Detection (POD), which measures the fraction of the observed events that were correctly forecasted ($POD=hits/(hits+misses)$). Therefore, POD only takes into account observed fires and, unlike other skill scores such as the Brier score, does not suffer from the artificial vanishing due to the high number of correct negative and false alarms (see  \citet{stephenson:08,ferro:11} for a discussion on these problems). 

\subsection{Fire regions}

The global assessment of the fire forecast skills  is mostly provided as an  average over selected regions even if the calculation of the various scores is performed at pixel level by interpolating the model grid over the benchmarks. For an assessment at the continental scale, we use the fire macro-regions defined by the Global Fire Emission Database, GFED4 \citep{giglio:13}. These macro-regions are characterized by different fire regimes and are very roughly homogeneous in their burning emissions contribution \citep{giglio:13}. Inside these regions we also select 3 areas at national/regional level - California, Portugal and Chile - which experience recurrent intense fire episodes and saw major events taking place in 2017 (Figure \ref{fig:Figure1}). Events in these locations are also analyzed in detail. 

```{r packages, echo = FALSE, message = FALSE, results = 'hide'}
# Install caliver from GitHub
# install.packages("devtools", repos = "http://cran.rstudio.com")
# devtools::install_github("ecmwf/caliver")

# Load relevant packages
library("caliver")
library("raster")
library("ggplot2")
library("rowr")
library("dplyr")
library("rgeos")
library("maptools")
library("sp")
library("sf")
library("RColorBrewer")
library("stringr")
library("colorspace")
library("easyVerification")
library("spatialEco")
library("doSNOW")
library("parallel")
library("gplots")

setwd("data")
forecasts_folder <- "/hugetmp/forecasts/hres/2017"

dates2017 <- seq.Date(from = as.Date("2017-01-01"),
                      to = as.Date("2017-12-31"),
                      by = "day")

datesERA5 <- seq.Date(from = as.Date("1980-01-01"),
                      to = as.Date("2019-06-30"),
                      by = "day")

# Get all GFED4 regions
BasisRegions <- readRDS("BasisRegions_simplified.rds")

# Add the id column to the GFED4 regions for join
regionXFort <- fortify(BasisRegions, data = BasisRegions@data)
regionXPoly <- merge(regionXFort, BasisRegions@data,
                     by.x = "id", by.y = "ID")  # join data
# Re-order factors
regionXPoly$Region <- factor(as.character(regionXPoly$Region),
                             levels = BasisRegions$Region)

# GFED4 regions of interest
EURO <- BasisRegions[BasisRegions$Region == "EURO",]
TENA <- BasisRegions[BasisRegions$Region == "TENA",]
SHSA <- BasisRegions[BasisRegions$Region == "SHSA",]

# Countries of interest
pt <- raster::getData(name = "GADM", country = "Portugal", level = 1)[-c(2,13),]
chile <- raster::getData(name = "GADM", country = "Chile", level = 0)
cali <- raster::getData(name = "GADM", country = "USA", level = 1)[5, ]
```

\section{Results}

\subsection{Skill in the FWI prediction}
The first assessment looks at the capability of ECMWF fire forecast to reproduce the same FWI values as would be estimated from the network of local stations but 2, 6 and 10 days ahead. The selected stations (figure \ref{fig:Figure2}) have at least 30 records during 2017 at local noon and are used to perform an analysis of bias and anomaly correlation at different lead times. For comparison also FWI calculation using ERA5 is included. This provide an estimations of the limit of predictability when using forcing from model simulations in place of observed values \citep{digiuseppe:16}. As expected there is a performance degradation going towards longer lead times and mean biases are limited to few units even at day 10. However, depending on the calibration procedure adopted few units could mean a mismatch in danger level classification.  The mean absolute error  (figure \ref{fig:synop}b)  provides  information on the residual amplitudes.  FWI from reanalysis have the largest skills as expected and the mean absolute errors rapidly increases with lead times. However the distribution of MAE values clearly shows that in selected cases predictive skills can be achievable even at day 10.

```{r prep, echo = FALSE, message = FALSE, results = 'hide', cache = TRUE, eval = FALSE}
# The following code uses forecast data which is restricted.
# A summary table is computed and saved for users at the end of this code chunk.
#
# Get data from previous publication
# https://github.com/cvitolo/GEFF-ERA5/blob/master/data/df_geff_erai_era5.rds
# save it in the "../data" folder
df <- readRDS(file = "df_geff_erai_era5.rds")
# insert 1-year fire season for tropics (lat = [-30, +30])
df$season[df$lat < 30 & df$lat > -30] <- "Dry"
# Remove wet season (we are only interested in the dry season)
df <- df[df$season == "Dry", ]
# How many stations remain?
stations <- unique(df[, c("id", "lat", "long")])

df$region <- sapply(strsplit(df$tzid, "/"), `[`, 1)
df$date <- as.Date(paste0(df$yr, "-", df$mon, "-", df$day))
df <- df[, c("id", "lat", "long", "region", "date", "OBS", "ERA5")]

days_2017 <- which(datesERA5 %in% c(dates2017, tail(dates2017)[6] + 1:9))
# system(paste0("cdo seltimestep,", paste0(days_2017, collapse = ","),
#                   " /scratch/rd/nen/perClaudia/era5/fwi_1980_2019.nc ",
#                   "/scratch/rd/nen/perClaudia/era5/fwi_2017_dayx.nc"),
#            ignore.stderr = TRUE)
ERA5_2017 <- raster::brick("/scratch/rd/nen/perClaudia/era5/fwi_2017_dayx.nc")

for (i in seq_along(dates2017)) {
  
  issue_date <- dates2017[i]

  # What stations have data on this date?
  dfx <- dplyr::filter(.data = df, date == issue_date)
  dfx <- dfx[complete.cases(dfx), ]

  # Define spatial point
  spdf <- sf::st_as_sf(x = dfx, coords = c("long", "lat"), crs = 4326)

  # Get HRES
  message(paste("Handling HRES for", issue_date))
  HRES <- raster::brick(file.path(forecasts_folder,
                                  paste0("ECMWF_FWI_",
                                         gsub("-", "",
                                              as.character(issue_date)),
                                         "_1200_hr_fwi.nc")))
  # HRES needs to be rotated
  df_hres <- raster::extract(x = raster::rotate(HRES), y = spdf)
  dfx <- cbind(dfx, df_hres)
  names(dfx)[8:17] <- paste0("HRES_d", 1:10)
  
  # Get ENS
  arr_ens <- array(NA, dim = c(dim(spdf)[1], 10, 51))
  message(paste("Handling ENS for", issue_date))
  for (j in 1:51){
    # Extract the modelled FWI from ENS
    ens_member <- sprintf("%02d", j - 1)
    ENS <- raster::brick(file.path(forecasts_folder,
                                   paste0("ECMWF_FWI_",
                                          gsub("-", "",
                                               as.character(issue_date)),
                                          "_1200_", ens_member,
                                          "_fwi.nc")))[[1:10]]
    arr_ens[, , j] <- raster::extract(x = raster::rotate(ENS), y = spdf)
  }
  
  # Get ERA5 data for 2017 - to be used as observation
  message(paste("Handling ERA5 for", issue_date, "plus 9 days"))
  ERA5_2017_day <- ERA5_2017[[which(dates2017 %in% issue_date) + 0:9]]
  day_in_2017 <- raster::extract(x = ERA5_2017_day, y = spdf)

  for (leadtime in 1:10){

    lead_day <- issue_date + leadtime - 1

    # Extract ERA5 from all the years (same month and day), excluding 2017
    idx_in_datesERA5 <- which(format(datesERA5, "%m-%d") %in%
                              format(lead_day, "%m-%d") &
                              lubridate::year(datesERA5) != "2017")
    system(paste0("cdo seltimestep,", paste0(idx_in_datesERA5, collapse = ","),
                  " /scratch/rd/nen/perClaudia/era5/fwi_1980_2019.nc ",
                  "/scratch/rd/nen/perClaudia/era5/fwi_1980_2019_lead_day.nc"),
           ignore.stderr = TRUE)
    ERA5 <- raster::brick("/scratch/rd/nen/perClaudia/era5/fwi_1980_2019_lead_day.nc")

    # ERA5 was stored with longitudes already rotated to [-180, +180],
    # HRES and ENS will need to be rotated
    df_era5_39 <- raster::extract(x = ERA5, y = spdf)
    # Sample ERA5 to get 51 random ensemble members
    df_era5_51 <- df_era5_39[, sample(x = 1:dim(df_era5_39)[2],
                                      size = 51, replace = TRUE)]

    # Populate CRPS
    crps_fc <- easyVerification::veriApply(verifun = 'EnsCrps',
                         fcst = arr_ens[, leadtime,],
                         obs = day_in_2017[, leadtime])
    crps_clim <- easyVerification::veriApply(verifun = 'EnsCrps',
                           fcst = df_era5_51,
                           obs = day_in_2017[, leadtime])
    crpss <- 1 - crps_fc/crps_clim
    
    dfx <- cbind(dfx, crps_fc, crps_clim, crpss)
  }
  names(dfx)[18:47] <- paste0(c("CRPS_fc_d", "CRPS_clim_d", "CRPSS_d"),
                              rep(1:10, each = 3))

  if (i == 1) {
    result <- dfx
  }else{
    result <- rbind(result, dfx)
  }

  saveRDS(result, "df_geff_era5_hres_crps.rds")

}
```

```{r prep2, echo = FALSE, message = FALSE, results = 'hide', cache = TRUE}
result <- readRDS("df_geff_era5_hres_crps.rds")

# Remove rows with NAs and update spatial points
rows2remove <- which(rowSums(is.na(result)) > 0)
if (length(rows2remove) > 0) result <- result[-rows2remove, ]

# Define spatial point
spdf <- sf::st_as_sf(x = result, coords = c("long", "lat"), crs = 4326)

synops <- spatialEco::point.in.poly(spdf, BasisRegions)
```

```{r Figure1, echo = FALSE, fig.width = 13, fig.height = 6, out.width = "100%", fig.cap = "\\label{fig:Figure1}GFED4 regional classification and the 3 countries selected to showcase the fire forecast performances (California, Chile, and Portugal)."}
ggplot(data = map_data("world"), aes(x = long, y = lat)) +
  geom_polygon(aes(group = group), fill=NA, colour = "grey65") +
  coord_equal() +  theme_bw() + xlab("Longitude") + ylab("Latitude") +
  geom_polygon(data = regionXPoly,
               aes(x = long, y = lat, group = group, fill = factor(Region)),
               alpha = 0.5) +
  scale_fill_manual(name="GFED4 regions",
                    values = c("darkgrey",
                               RColorBrewer::brewer.pal(12,"Paired"),
                               "lightgrey"),
                    labels = BasisRegions$Region) +
  geom_polygon(data = map_data("world", region = "Portugal"),
               aes(x = long, y = lat, group = group, colour = "Portugal"),
               fill = NA) +
  geom_polygon(data = map_data("world", region = "Chile"),
               aes(x = long, y = lat, group = group, colour = "Chile"),
               fill = NA) +
  geom_polygon(data = map_data(map = "state", region = "California"),
               aes(x = long, y = lat, group = group, colour = "California"),
               fill = NA) +
  geom_point(data = data.frame(synops), aes(x = coords.x1, y = coords.x2),
             shape = 1, color = "gray20", size = 0.1) +
  scale_colour_manual(name = "Study areas",
                      values = c("Portugal" = "brown",
                                 "California" = "darkblue",
                                 "Chile" = "darkgreen"))
```

```{r Figure2, echo = FALSE, fig.width = 13, fig.height = 10, out.width = "100%", fig.cap = "\\label{fig:Figure3}Comparison between modelled FWI and observed FWI value. FWI are calculated from using ECMWF reanalysis (ERA5) and forecasts at different lead times. The box plots are used to describe the distribution of values accross the observation points for the 2017 Mean Bias (left panel) and the 2017 mean absolute error (right panel)."}
f2 <- data.frame(synops) %>%
  group_by(Region) %>%
  select(OBS, Region, ERA5, HRES_d1, HRES_d2, HRES_d3, HRES_d4, HRES_d5,
         HRES_d6, HRES_d7, HRES_d8, HRES_d9, HRES_d10) %>%
  reshape2::melt(id.vars = c("OBS", "Region")) %>%
  filter(complete.cases(.)) %>% # Remove NAs
  filter(OBS < 250, value < 250) %>%
  mutate(BIAS = OBS - value,
         MAE = abs(OBS - value)) %>%
  rename(modelled_type = variable, modelled_value = value) %>%
  reshape2::melt(id.vars = c("OBS", "Region", "modelled_type",
                             "modelled_value"))

ggplot(f2, aes(x = modelled_type, y = value)) +
  facet_grid(Region ~ variable) +
  geom_boxplot(color="black", outlier.shape = NA) +
  theme_bw() + ylim(-40, 50) +
  xlab("") + ylab("") +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("ERA5", "D1", "D2", "D3",
                              "D4", "D5", "D6", "D7",
                              "D8", "D9", "D10"))
```

Despite its importance the analysis performed using the synop network is still pointwise and does not cover all the regions where fires are relevant. Moreover mean biases and MAE are based on the high resolution forecast. These skill metrics do not provide information about the performance of the ensemble forecasting as a whole. 

```{r Figure3, echo = FALSE, fig.width = 13, fig.height = 6, out.width = "100%", fig.cap = "\\label{fig:Figure4}PUT here crps skills scores."}
f3 <- data.frame(synops) %>%
  select(Region, CRPS_fc_d1, CRPS_fc_d2, CRPS_fc_d3, CRPS_fc_d4, CRPS_fc_d5, CRPS_fc_d6,
         CRPS_fc_d7, CRPS_fc_d8, CRPS_fc_d9, CRPS_fc_d10) %>%
  reshape2::melt(id.vars = "Region") %>%
  filter(complete.cases(.)) %>%
  rename(CRPS = value) %>%
  group_by(Region, variable) %>%
  summarise(CRPS = mean(CRPS))

f3$variable <- as.numeric(f3$variable)
f3$Region <- factor(f3$Region)

ggplot(f3, aes(x = variable, y = CRPS, colour = Region)) +
  geom_line() +
  geom_point() +
  xlab("Lead time") +
  theme_bw() +
  scale_x_discrete(limits = 1:10) +
  scale_colour_manual(name="Regions",
                      values = c("darkgrey",
                                 RColorBrewer::brewer.pal(12,"Paired"),
                                 "lightgrey"),
                      labels = BasisRegions$Region)
```

```{r Figure4, echo = FALSE, fig.width = 13, fig.height = 6, out.width = "100%", fig.cap = "\\label{fig:Figure4}PUT here crps skills scores."}
f4 <- data.frame(synops) %>%
  filter(complete.cases(.)) %>%
  select(paste0(c("CRPS_fc_d", "CRPS_clim_d"), rep(1:10, each = 2))) %>%
  reshape2::melt(id.vars = c("CRPS_clim_d1", "CRPS_clim_d2", "CRPS_clim_d3",
                             "CRPS_clim_d4", "CRPS_clim_d5", "CRPS_clim_d6",
                             "CRPS_clim_d7", "CRPS_clim_d8", "CRPS_clim_d9",
                             "CRPS_clim_d10")) %>%
  rename(CRPS_fc = value) %>%
  rename(fc_name = variable) %>%
  reshape2::melt(id.vars = c("fc_name", "CRPS_fc")) %>%
  rename(CRPS_clim = value) %>%
  rename(clim_name = variable) %>%
  group_by(fc_name, clim_name) %>%
  summarise(CRPS_fc = mean(CRPS_fc),
            CRPS_clim = mean(CRPS_clim))

f4$fc_name <- as.numeric(f4$fc_name)
f4$clim_name <- as.numeric(f4$clim_name)

f4plot <- ggplot(f4) +
  geom_line(aes(x = fc_name, y = CRPS_fc, col = "Forecast"), size = 2) +
  geom_line(aes(x = clim_name, y = CRPS_clim, col = "Climatology"), size = 2) +
  xlab("Lead time") + ylab("CRPS") +
  scale_color_discrete(name = "") +
  theme_bw() +
  scale_x_discrete(limits = 1:10) +
  theme(text = element_text(size = 15))

ggsave(filename = "../images/Figure4.png", plot = f4plot)
```

```{r Figure5, echo = FALSE, fig.width = 13, fig.height = 6, out.width = "100%", fig.cap = "\\label{fig:Figure4}PUT here crps skills scores."}
f5 <- data.frame(synops) %>%
  filter(complete.cases(.)) %>%
  select(Region, paste0(c("CRPS_fc_d", "CRPS_clim_d"), rep(1:10, each = 2))) %>%
  reshape2::melt(id.vars = c("Region",
                             "CRPS_clim_d1", "CRPS_clim_d2", "CRPS_clim_d3",
                             "CRPS_clim_d4", "CRPS_clim_d5", "CRPS_clim_d6",
                             "CRPS_clim_d7", "CRPS_clim_d8", "CRPS_clim_d9",
                             "CRPS_clim_d10")) %>%
  rename(CRPS_fc = value) %>%
  rename(fc_name = variable) %>%
  reshape2::melt(id.vars = c("Region", "fc_name", "CRPS_fc")) %>%
  rename(CRPS_clim = value) %>%
  rename(clim_name = variable) %>%
  group_by(Region, fc_name, clim_name) %>%
  summarise(CRPS_fc = mean(CRPS_fc),
            CRPS_clim = mean(CRPS_clim))

f5$fc_name <- as.numeric(f5$fc_name)
f5$clim_name <- as.numeric(f5$clim_name)
f5$Region <- factor(f5$Region)

ggplot(f5) +
  geom_line(aes(x = fc_name, y = CRPS_fc, col = "Forecast")) +
  geom_line(aes(x = clim_name, y = CRPS_clim, col = "Climatology")) +
  facet_wrap( ~ Region, ncol = 2, scales = "free_y") +
  xlab("Lead time") + ylab("CRPS") +
  scale_color_discrete(name = "") +
  theme_bw() +
  scale_x_discrete(limits = 1:10)
```

\subsection{Skill in detecting fire events}

Conventionally, we assume that an active fire is correctly predicted if the FWI is greater than the 95$^{th}$ percentile of its distribution of values here defined using the ERA5 database. Figure~\ref{fig:pod} shows the mean POD for all events in 2017 at forecast day 2, 6 and day 10. Figure~\ref{fig:pod} also shows the POD that could be achieved in the absence of a forecasting system when just using a mean climatological FWI estimation. Given the intrinsic limitations of the POD as skill metric, CLIM provides an useful benchmark to understand the incremental skill provided by the forecast. 

At day 2, high latitudes are characterized by POD greater than or equal to 0.5. These are mostly temperate regions where vegetation is dominated by forests and fuel is abundant and where fire danger is moisture limited. In these regions the FWI is a good predictor of fire danger \citep{digiuseppe:16}. It has to be noted that the FWI does not take into account management measures that could introduce a relevant number of "false-alarm". Central America, the Middle East and the northern hemisphere areas, Africa are characterized by a POD in the range 0.3-0.4 as in most of the tropics, where, fires usually occur in grass-shrub lands. Here fuel is scarce and weather plays a less relevant controlling role. Also it has to be noted that  the statistics here are likely to be contaminated by  many agricultural and prescribed fires that are considered “events” and which would dilute some of the skill in regions where annual cropland is high or are heavily managed. The spatial distribution of POD at day 6 is very similar to the corresponding figures at day 2, just slightly shifted towards lower values. 

One important exception is the extremely good performance of the fire forecast in Equatorial Asia where the system seems to have a predictability of 0.9 even at day 6. Also \citet{degroot:07} highlighted how FWI is not a good indicator in this area and a fire early warning system should mostly rely on the drought code. Still, there are factors  that could contribute to this enhanced predictability. First fires in this region are mainly caused by humans for the purposes of cleaning the land for establishing plantations \citep{field:09,benedetti:16}. Therefore they occur every season as soon as fuel conditions are favorable and the ignition is less of a stochastic component. This also means that the POD statistics are built on a much higher number of events. Moreover, the strength and prevalence of these fires are strongly influenced by large-scale climate patterns like El Ni{\~n}o \cite{field:04} which have been proven being highly predictable \citep{zhu:15}. The other relevant fact is the almost null skill globally when a climatological FWI is used highlighting the added benefit of a forecasting system

```{r prepFig6_7, eval = FALSE, echo = FALSE, message = FALSE, results = 'hide', cache = TRUE}
# Get ERA5
ERA5 <- raster::brick("/scratch/rd/nen/perClaudia/era5/fwi_1980_2019.nc")
# Get FRP
FRP <- raster::brick("/scratch/rd/nen/perClaudia/CAMS/CAMS_2017-01-01_2017-12-31_frpfire.nc")
myfilelist <- list.files(path = forecasts_folder,
                        pattern = "*hr",
                        full.names = TRUE)

ncores <- parallel::detectCores() - 1
cl <- makeCluster(ncores)
registerDoSNOW(cl)
iterations <- 365
pb <- txtProgressBar(max = iterations, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
result <- foreach(i = 10:iterations, .combine = rbind, 
                  .options.snow = opts) %dopar%
{
  # Get FRP for 2017, keep only major fires (values above 0.5)
  frpx <- raster::rotate(FRP[[i]])
  p <- raster::rasterToPoints(frpx, fun = function(x){x > 0.5})
  spdf <- data.frame(p)
  spdf$date <- dates2017[i]
  names(spdf) <- c("long", "lat", "frp", "date")
  sp::coordinates(spdf) <- ~long + lat
  n_points <- dim(spdf@data)[1]
  
  # Get HRES for 2017, where FRP is above 0.5
  lead_day <- 0
  hres_df <- data.frame(matrix(NA, ncol = 10, nrow = n_points))
  for (j in i:(i - 9)){
    lead_day <- lead_day + 1
    hres <- raster::rotate(raster::brick(myfilelist[[j]])[[lead_day]])
    x <- raster::extract(x = hres, y = spdf)
    hres_df[, lead_day] <- x
  }
  names(hres_df) <- paste0("hres_day", 1:10)
  
  # Get ERA5 to calculate various warning levels
  # Low = 0.75, Moderate = 0.85, High = 0.90, Very high = 0.95, Extreme = 0.98
  median <- caliver::daily_clima(r = ERA5, dates = dates2017[i], probs = 0.50)
  moderate <- caliver::daily_clima(r = ERA5, dates = dates2017[i], probs = 0.85)
  high <- caliver::daily_clima(r = ERA5, dates = dates2017[i], probs = 0.90)
  very_high <- caliver::daily_clima(r = ERA5, dates = dates2017[i], probs = 0.95)
  CLIM <- raster::extract(x = median, y = spdf)
  thr85 <- raster::extract(x = high, y = spdf)
  thr90 <- raster::extract(x = high, y = spdf)
  thr95 <- raster::extract(x = very_high, y = spdf)

  # Append results to table
  df <- cbind(spdf@coords, spdf@data, CLIM, thr85, thr90, thr95, hres_df)
  df <- df[complete.cases(df), ]
  return(df)
}
close(pb)
stopCluster(cl)
# saveRDS(result, "PODdataframe.rds")

# Load, if pre-calculate
# result <- readRDS("PODdataframe.rds")
# BasisRegions <- readRDS("BasisRegions_simplified.rds")

# FRP locations of interest
sp::coordinates(result) <- ~long + lat
crs(result) <- crs(BasisRegions)

# Add ID, Region and Zone to the dataset vias spatial intersection
fires <- spatialEco::point.in.poly(result, BasisRegions)
fires <- as.data.frame(fires)
fires <- fires[complete.cases(fires), ]

# Check coverage
# plot(BasisRegions)
# plot(result, col = "red", add = TRUE)

# If all looks good, we can remove previous datasets
# rm(result, BasisRegions)

# saveRDS(fires, "fires.rds")
```

```{r prepFig6, eval = FALSE, echo = FALSE, message = FALSE, results = 'hide', cache = TRUE}
# fires <- readRDS("fires.rds")

# Check if CLIM and HRES are above threshold (90th percentile)
my_thr <- fires$thr90
fires$CLIM <- fires$CLIM >= my_thr
for (colx in which(names(fires) %in% paste0("hres_day", 1:10))){
  x <- fires[, colx] > my_thr
  fires <- cbind(fires, x)
}
names(fires)[which(names(fires) == "x")] <- paste0("HDay", 1:10)
rm(x, colx)

# Group by Region and calculate POD
fires_binary <- fires %>%
  mutate(FRP_binary = TRUE) %>%
  select(Region, coords.x1, coords.x2, FRP_binary, CLIM,
         HDay1, HDay2, HDay3, HDay4, HDay5,
         HDay6, HDay7, HDay8, HDay9, HDay10) %>%
  reshape2::melt(id.vars = c("Region", "coords.x1", "coords.x2", "FRP_binary")) %>%
  group_by(Region, coords.x1, coords.x2, variable) %>%
  summarise(POD = verify(obs = FRP_binary, pred = value,
                         frcst.type = "binary", obs.type = "binary")$POD) %>%
  group_by(Region, variable) %>%
  summarise(PODmean = mean(POD))
# saveRDS(fires_binary, "fires_binary_90.rds")
```

```{r Figure6, echo = FALSE, fig.width = 13, fig.height = 6, out.width = "100%", fig.cap = "\\label{fig:Figure5}Global area averaged Probability of Detection (POD) for  day 2, day 6 and day 10 forecasts and the CLIM run. Pixels where FRP $ \ge $ 0.5 Wm$^{-2}$  are categorized as "yes" events and compared to FWI prediction above the very-high warning level. The global statistic is constructed using all FRP observations detected in 2017 and averaged over the specified regions."}
fires_binary <- readRDS("fires_binary_90.rds")
fires_binary <- fires_binary[-which(fires_binary$variable == "CLIM"), ]

ggplot(fires_binary, aes(variable, Region, fill = PODmean)) + 
  geom_tile() +
  geom_text(aes(label = round(PODmean, 2))) +
  scale_fill_gradient(name = "POD", low = "white", high = "brown") +
  theme_bw() + xlab("") + ylab("")
```

```{r Figure7, echo = FALSE, fig.width = 13, fig.height = 6, out.width = "100%", fig.cap = "\\label{fig:Figure4}GFED4 regional classification and the locations of events reported as Fire Radiative Power (FRP) by GFAS."}
ggplot(data = map_data("world"), aes(x = long, y = lat)) +
  geom_polygon(aes(group = group), fill=NA, colour = "grey65") +
  coord_equal() +  theme_bw() + xlab("Longitude") + ylab("Latitude") +
  geom_polygon(data = regionXPoly,
               aes(x = long, y = lat, group = group, fill = factor(Region)),
               alpha = 0.5) +
  scale_fill_manual(name="GFED4 regions",
                    values = c("darkgrey",
                               RColorBrewer::brewer.pal(12,"Paired"),
                               "lightgrey"),
                    labels = BasisRegions$Region) +
  geom_point(data = data.frame(result), aes(x = long, y = lat),
             shape = 1, color = "gray20", size = 0.1)
```

\subsection{2017 case studies}

Figure \ref{fig:pod} provides an averaged assessment of the global performances of the forecasted FWI as a generic indicator of fire danger. There are regional and seasonal variations to this skill. Also it is important to understand how the information provided could be used in real cases when the forecast is intended to aid emergency responses. Here we will analyses three cases of fire events that took place in 2017, which proved to be an extreme year for fire all across the globe. 

The year 2017 started with an extended fire in central Chile that lasted almost all of January. Strong winds, high temperatures and long-term drought conditions led to an event that has been described as the worst wildfire in Chilean history \citep{bowman:18}. Fires in the central regions of O’Higgins, Maule and B\'io B\'io south of Santiago were difficult to control. Although fire activities where recorded since July 2016 they became particularly intense in January 2017.  In June, between 17 and 18, another devastating fire hit Portugal. It claimed more than 60 lives mostly recorded in the Pedr\'og\~ao Grande area, 50 km southeast of Coimbra. A persistent heatwave had been building in the region, with temperatures above 40C, which are highly unusual for the season. Moreover, relative humidity levels below 30\% had a role to the intensification of the deflagration and the spread of the wildfire, which raged out of control for several days \citep{boer:17}. Finally in Octoberextensive wildfires raced just north of the San Francisco Bay Area in California causing historic levels of death and destruction. These named  “Wine Country” wildfires were the most destructive in California history, with 44 deaths; the loss of 9,000 buildings; damage to approximately 21,000 structures; \$10 billion of insured losses; and substantially greater total economic loss \citep{nauslar:18,mass:19}. 

For each case study, the affected area is identified as the minimum area including all detected active fires (cells with $FRP > 0.5 Wm^{-2}$) during the selected time window. Figure \ref{fig:plotthatrulesthemall} shows the information that could have been provided for the study areas by the 10-day fire danger high resolution forecasts (HRES), had these been already available. Each plot shows on the x-axis the dates in which FRP was observed and, on the y-axis, the dates forecasts were issued. The cell in the bottom left corner shows the percentage of pixels in the study area that are expected to be above the 95$^{th}$ percentile of the FWI climatology for that  pixel. The forecasts for day 2 to day 10 are on the same row. The forecasts issued on the following day are one row above and so forth. The dashed lines shows the observed fire radiative power (see also secondary y-axis). 

The reader is reminded that active fires are triggered by highly unpredictable events (ignition) which are not accounted for in the FWI system. The FWI is not supposed to provide the exact localization of the event but an indication of potential fire activity. Large areas can be affected by anomalous conditions in the proximity of where the event really occurred. However it is noticeable the capability of the forecast to detect the increase in fire danger associated to the three  events. For the Chile case, for example from mid-January between 70 and 80\% of the area exceeded the high danger threshold. The FRP spike (occurred on 26th January) highlighting that most of the region was classified at very high danger almost 10 days ahead. Similarly, the Portugal fire  could have been predicted ten days before, but the California event only two to three days.

\begin{table*}
\caption{Events summary table.}
\label{tab:fires}
\begin{center}
\begin{tabular}{|llllll|}
\hline
Country  & Region          & Start date & End date &Main event & Location  \\\hline
Chile    & O'Higgins, Maule, B\'io B\'io  & 01-01-2017 & 31-01-2017& 26-01-2017& 36$^\circ$ 46'S; 73$^\circ$ 03'W \\
Portugal & Pedrogao Grande & 01-06-2017 & 30-06-2017 &18-06-2017 & 39$^\circ$ 55'N ; 8$^\circ$ 08' W\\
USA      & California      & 21-09-2017 & 20-10-2017 &09-10-2017& 38$^\circ$ 34'N; 122$^\circ$ 34' W\\
\hline
\end{tabular}
\end{center}
\end{table*}

```{r prepFig6_maps, eval = FALSE, echo = FALSE, message = FALSE, results = 'hide', cache = TRUE}
# Chile
dates_chile <- seq.Date(from = as.Date("2017-01-01"),
                        to = as.Date("2017-01-31"),
                        by = "day")
bbox_chile <- as(raster::extent(-74, -70, -37, -33), "SpatialPolygons")
raster::plot(chile, col = "lightgray")
raster::plot(bbox_chile, lwd = 2, border = "red", add = TRUE)
# Crop manually to mainland!

# Portugal
dates_pt <- seq.Date(from = as.Date("2017-06-01"),
                     to = as.Date("2017-06-30"),
                     by = "day")
bbox_pt <- as(raster::extent(-8.5, -7.6, 39.6, 40.3), "SpatialPolygons")
pt <- gUnaryUnion(pt, id = pt@data$NAME_0)
raster::plot(pt, col = "lightgray")
raster::plot(bbox_pt, lwd = 2, border = "red", add = TRUE)
# Crop manually to mainland!

# California
dates_cali <- seq.Date(from = as.Date("2017-09-21"),
                       to = as.Date("2017-10-20"),
                       by = "day")
bbox_cali <- as(raster::extent(-123.4, -117.6, 33.7, 42), "SpatialPolygons")
raster::plot(cali, col = "lightgray")
raster::plot(bbox_cali, lwd = 2, border = "red", add = TRUE)
# Crop manually to mainland!

obs_file <- "/scratch/rd/nen/perClaudia/CAMS/CAMS_2017-01-01_2017-12-31_frpfire.nc"
clima_file <- "/scratch/rd/nen/perClaudia/era5/fwi_era5_1980_2016_90th_daily_clima.nc"

# Chile
df_chile <- caliver:::make_forecast_summary(input_dir = "/hugetmp/forecasts/fwi",
                                            p = bbox_chile,
                                            event_dates = dates_chile,
                                            obs = obs_file,
                                            clima = clima_file)
saveRDS(df_chile, "df_chile.rds")
# Portugal
df_pt <- caliver:::make_forecast_summary(input_dir = "/hugetmp/forecasts/fwi",
                                         p = bbox_pt,
                                         event_dates = dates_pt,
                                         obs = obs_file,
                                         clima = clima_file)
saveRDS(df_pt, "df_pt.rds")
# California
df_cali <- caliver:::make_forecast_summary(input_dir = "/hugetmp/forecasts/fwi",
                                           p = bbox_cali,
                                           event_dates = dates_cali,
                                           obs = obs_file,
                                           clima = clima_file)
saveRDS(df_cali, "df_cali.rds")
```

```{r Figure6, echo = FALSE, fig.width = 13, fig.height = 6, out.width = "100%", fig.cap = "\\label{fig:Figure6a}Chile."}
# Chile
plot_chile <- caliver:::plot_forecast_summary(df_chile)
# Portugal
# df_pt <- readRDS("df_pt.rds")
plot_pt <- caliver:::plot_forecast_summary(df_pt)
# California
# df_cali <- readRDS("df_cali.rds")
plot_cali <- caliver:::plot_forecast_summary(df_cali)

#\caption{Comparison of Fire Radiative Power (gray dashed line with axis on the right hand side) with FWI forecasted using the deterministic high resolution model for California, Portugal and  Chile. FWI is color coded based on the percentage of pixels exceeding the high danger level calculated at the country/state level. Each of the panel refers to a specific fire event described in the text and the statistics have been calculated over the red boxes.} \label{fig:plotthatrulesthemall}
```

One of the advantages of developing a probabilistic ensemble prediction system (ENS) for fire danger is the possibility to assess the confidence in the model forecast, at least from the point of view of the uncertainties in weather forcings. Admittedly, the use of this type of information is not straightforward as many studies have highlighted \citep{pappenberger:13,palmer:00}. This difficulty stems from the fact that in many response systems an activation is conditional to the exceedance of  deterministic threshold. Instead, in a probabilistic approach what is provided is the {\it probability} of exceedance a certain values and this is often perceived as a degradation of the information values \citep{richardson:00}. However by using the probabilistic component of the system, it is possible to understand how consistent the fire danger forecast is with respect to slightly different forecast scenarios. 

\conclusions

Using one year of operational service in 2017 we have showcased the potential of the use of weather forecasts to support the monitoring of fire danger conditions and planning in case of a potential emergency. Using the three large fire events of 2017 in Chile, Portugal and California, as examples, we have shown that accurate forecasts can be achieved up to 10 days ahead. Another interesting aspect attached to the use of weather forecasts is the use of probabilistic information. The quantification of forecast uncertainties through the use of ensemble predictions is something still pretty new in fire forecasting. However it opens great opportunities in terms of adding a confidence level to the the fire prediction.
